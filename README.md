# LLM-Basics

## Pre-training phase - Decoder from scratch (Andrej Karpathy)
1. Implemented video code (Andrej Karpathy)

video Link: [https://www.youtube.com/watch?v=kCc8FmEb1nY](https://www.youtube.com/watch?v=kCc8FmEb1nY)

2. Tried with Harry potter dataset (1 book)
3. Tried with all 7 books- decent results
4. increased batch-size (context length) and used frequency encoding (character based) - better loss (1.8)
5. Increased epochs - better results (final loss - 1.4 )
6. Tried bert based encoding and decoding 

    After 1 epoch loss = 10.47

    After 100 epochs loss = 6.31

    After 300 epochs loss = 5.59

## Supervised Fine-tuning phase - Phi2 with qlora
Will update soon xD
